{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# advent of code 2024 - [day 9](https://adventofcode.com/2024/day/9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(filename='input.txt'):\n",
    "    \"\"\"Generates tuples of integers\"\"\"\n",
    "    file = open(filename, 'r')\n",
    "    line = file.readline().strip()\n",
    "    for ix, c in enumerate(line.strip()):\n",
    "        yield ix, \"File\" if ix%2 == 0 else \"FreeSpace\", int(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import doctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"input.txt\"\n",
    "#filename = \"test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "NEO4J_URI = os.environ['NEO4J_URI']\n",
    "NEO4J_USERNAME = os.environ['NEO4J_USERNAME']\n",
    "NEO4J_PASSWORD = os.environ['NEO4J_PASSWORD']\n",
    "\n",
    "from graphdatascience import GraphDataScience\n",
    "gds = GraphDataScience(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "# Check the installed GDS version on the server\n",
    "print(gds.version())\n",
    "assert gds.version()\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def infer (rules, params={}):\n",
    "    \"\"\"\n",
    "    This is a function you can use if you want to run a set of inference rules\n",
    "    until a convergence is reached. why not use it in a RDF-like reasoning context?\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        any_update = False\n",
    "        for rule in rules:\n",
    "            with driver.session(database=\"neo4j\") as session:\n",
    "                result = session.run(rule, params)\n",
    "            any_update = any_update or result.consume().counters._contains_updates\n",
    "        if not any_update:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "def clean():\n",
    "    queries = [\n",
    "    \"CALL apoc.schema.assert({},{});\",\n",
    "    \"\"\"MATCH (n)\n",
    "    CALL {WITH n DETACH DELETE n}\n",
    "    IN TRANSACTIONS OF 1000 ROWS;\"\"\",\n",
    "    \"\"\"CALL gds.graph.list()\n",
    "    YIELD graphName\n",
    "    WITH graphName AS g\n",
    "    CALL (g) {CALL gds.graph.drop(g) YIELD graphName RETURN graphName}\n",
    "    WITH graphName RETURN graphName;\"\"\"]\n",
    "\n",
    "    for q in queries:\n",
    "        gds.run_cypher(q, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest(filename):\n",
    "\n",
    "    block_seq_list = list({'ix':ix, 'block_type':block_type, 'len':len} for ix, block_type, len in gen_data(filename))\n",
    "    \n",
    "    clean()\n",
    "\n",
    "    gds.run_cypher('CREATE INDEX block_seq_ix IF NOT EXISTS FOR (bs:BlockSeq) ON (bs.ix)')\n",
    "    gds.run_cypher('CREATE INDEX freespace_ix IF NOT EXISTS FOR (bs:FreeSpace) ON (bs.ix)')\n",
    "    gds.run_cypher('CREATE INDEX file_ix IF NOT EXISTS FOR (bs:File) ON (bs.ix)')\n",
    "\n",
    "    queries = [\n",
    "    \"\"\"\n",
    "    UNWIND $block_seq_list AS block_seq\n",
    "    CREATE (bs:BlockSeq {ix: block_seq.ix} )\n",
    "    SET bs.len = block_seq.len, bs:$(block_seq.block_type)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    MATCH (bs:BlockSeq)\n",
    "    WITH bs ORDER BY bs.ix\n",
    "    WITH collect(bs) AS bss\n",
    "    CALL apoc.nodes.link(bss, \"NEXT_BLOCK_SEQ\")\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    MATCH (bs:BlockSeq&File)\n",
    "    SET bs.id_number = bs.ix/2\n",
    "    \"\"\",\n",
    "    \"\"\"MATCH (bs:BlockSeq)\n",
    "    WHERE NOT EXISTS {()-[:NEXT_BLOCK_SEQ]->(bs)}\n",
    "    SET bs:First, bs:CurrentFirst\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    MATCH (bs:BlockSeq)\n",
    "    WHERE NOT EXISTS {(bs)-[:NEXT_BLOCK_SEQ]->()}\n",
    "    SET bs:Last, bs:CurrentLast\n",
    "    \"\"\"\n",
    "    ]\n",
    "\n",
    "    for q in queries:\n",
    "        gds.run_cypher(q, {\"block_seq_list\":block_seq_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_queries =[\n",
    "\"\"\"\n",
    "MATCH path=(first:CurrentFirst&File)((ni:File&!Deactivated)-[:NEXT_BLOCK_SEQ]->())*(ffree:FreeSpace&!Deactivated),\n",
    "  (lfile:File&!Deactivated)\n",
    "  (()-[:NEXT_BLOCK_SEQ]->(:FreeSpace&!Deactivated))*(last:CurrentLast)\n",
    "WHERE lfile.len < ffree.len\n",
    "MATCH (bffree)-[:NEXT_BLOCK_SEQ]->(ffree)-[:NEXT_BLOCK_SEQ]->(affree)\n",
    "REMOVE lfile:File, first:CurrentFirst, last:CurrentLast\n",
    "SET lfile:FreeSpace, lfile:CurrentLast, ffree:Deactivated\n",
    "CREATE (a:BlockSeq:File:CurrentFirst {len: lfile.len, id_number: lfile.id_number})-[:NEXT_BLOCK_SEQ]->(b:BlockSeq:FreeSpace {len: ffree.len - lfile.len})\n",
    "MERGE (bffree)-[:NEXT_BLOCK_SEQ]->(a)\n",
    "MERGE (b)-[:NEXT_BLOCK_SEQ]->(affree);\n",
    "\"\"\",\"\"\"\n",
    "MATCH path=(first:CurrentFirst&File)((ni:File&!Deactivated)-[:NEXT_BLOCK_SEQ]->())*(ffree:FreeSpace&!Deactivated),\n",
    "  (lfile:File&!Deactivated)\n",
    "  (()-[:NEXT_BLOCK_SEQ]->(:FreeSpace&!Deactivated))*(last:CurrentLast)\n",
    "WHERE lfile.len > ffree.len\n",
    "REMOVE ffree:FreeSpace, first:CurrentFirst\n",
    "SET lfile.len = lfile.len - ffree.len, ffree.id_number = lfile.id_number, ffree:File, ffree:CurrentFirst\n",
    "\"\"\",\"\"\"\n",
    "MATCH path=(first:CurrentFirst&File)((ni:File&!Deactivated)-[:NEXT_BLOCK_SEQ]->())*(ffree:FreeSpace&!Deactivated),\n",
    "  (lfile:File&!Deactivated)\n",
    "  (()-[:NEXT_BLOCK_SEQ]->(:FreeSpace&!Deactivated))*(last:CurrentLast)\n",
    "WHERE lfile.len = ffree.len\n",
    "REMOVE ffree:FreeSpace, lfile:File, first:CurrentFirst, last:CurrentLast\n",
    "SET lfile:FreeSpace, lfile:CurrentLast, ffree:File, ffree:CurrentFirst, ffree.id_number = lfile.id_number\n",
    "\"\"\"\n",
    "]\n",
    "infer(infer_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6332189866718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           part1\n",
       "0  6332189866718"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher(\"\"\"MATCH path=(first:First&File)((ni:File&!Deactivated)-[:NEXT_BLOCK_SEQ]->())*(ffree:FreeSpace&!Deactivated)\n",
    "WITH apoc.coll.flatten([x IN ni | [_ IN range(1, x.len) | x.id_number]]) AS fileCompacting\n",
    "WITH reduce(acc={checksum:0, step:0}, x in fileCompacting| {checksum: acc.checksum + acc.step * x, step: acc.step + 1}) AS checksum_step\n",
    "RETURN checksum_step.checksum AS part1\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_queries =[\n",
    "\"\"\"\n",
    "MATCH (last:File&!Processed)\n",
    "WITH last ORDER BY last.ix DESC LIMIT 1\n",
    "SET last:Processed\n",
    "WITH last\n",
    "CALL (last) {\n",
    "  MATCH (free:FreeSpace&!Deactivated WHERE free.len >= last.len AND free.ix < last.ix)\n",
    "  WITH free, last ORDER BY free.ix ASC LIMIT 1\n",
    "  MATCH (bfree)-[:NEXT_BLOCK_SEQ]->(free)-[:NEXT_BLOCK_SEQ]->(afree)\n",
    "  CREATE (a:BlockSeq:File {len: last.len, id_number: last.id_number, ix:free.ix})-[:NEXT_BLOCK_SEQ]->(b:BlockSeq:FreeSpace {len: free.len - last.len, ix:free.ix})\n",
    "  MERGE (bfree)-[:NEXT_BLOCK_SEQ]->(a)\n",
    "  MERGE (b)-[:NEXT_BLOCK_SEQ]->(afree)\n",
    "  SET free:Deactivated, last:FreeSpace\n",
    "  REMOVE last:File\n",
    "}\n",
    "\"\"\"\n",
    "]\n",
    "infer(infer_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6353648390778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           part2\n",
       "0  6353648390778"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher(\"\"\"MATCH path=SHORTEST 1 (first:First)((ni:!Deactivated)-[:NEXT_BLOCK_SEQ]->())*(last:Last)\n",
    "WITH apoc.coll.flatten([x IN ni| [_ IN range(1, x.len) | CASE WHEN x:File&Processed THEN x.id_number ELSE 0 END]]) AS fileCompacting\n",
    "WITH reduce(acc={checksum:0, step:0}, x in fileCompacting| {checksum: acc.checksum + acc.step * x, step: acc.step + 1}) AS checksum_step\n",
    "RETURN checksum_step.checksum AS part2\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
