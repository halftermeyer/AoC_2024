{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# advent of code 2024 - [day 15](https://adventofcode.com/2024/day/15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_map(file='input.txt'):\n",
    "    file = open(file, 'r')\n",
    "    for row, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            break\n",
    "        for col, val in enumerate(tuple(line)):\n",
    "            yield row, col, val\n",
    "        \n",
    "\n",
    "def gen_moves(file='input.txt'):\n",
    "    file = open(file, 'r')\n",
    "    state = \"wait\"\n",
    "    new_state = state\n",
    "    for ix, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        state = new_state\n",
    "        if line == '':\n",
    "            new_state = 'go'\n",
    "        if state == 'wait':\n",
    "            continue\n",
    "        for move in tuple(line):\n",
    "            yield move\n",
    "\n",
    "\n",
    "def gen_map_part2(file='input.txt'):\n",
    "    file = open(file, 'r')\n",
    "    for row, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            break\n",
    "        for col, val in enumerate(tuple(line)):\n",
    "            yield row, 2*col, {'#':'#', '@':'@', 'O':'[', '.':'.'}[val]\n",
    "            yield row, 2*col+1, {'#':'#', '@':'.', 'O':']', '.':'.'}[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "NEO4J_URI = os.environ['NEO4J_URI']\n",
    "NEO4J_USERNAME = os.environ['NEO4J_USERNAME']\n",
    "NEO4J_PASSWORD = os.environ['NEO4J_PASSWORD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "gds = GraphDataScience(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "# Check the installed GDS version on the server\n",
    "print(gds.version())\n",
    "assert gds.version()\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "def clean():\n",
    "    queries = [\n",
    "    \"CALL apoc.schema.assert({},{});\",\n",
    "    \"\"\"MATCH (n)\n",
    "    CALL {WITH n DETACH DELETE n}\n",
    "    IN TRANSACTIONS OF 1000 ROWS;\"\"\",\n",
    "    \"\"\"CALL gds.graph.list()\n",
    "    YIELD graphName\n",
    "    WITH graphName AS g\n",
    "    CALL (g) {CALL gds.graph.drop(g) YIELD graphName RETURN graphName}\n",
    "    WITH graphName RETURN graphName;\"\"\"]\n",
    "\n",
    "    for q in queries:\n",
    "        gds.run_cypher(q, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest(filename):\n",
    "    clean()\n",
    "\n",
    "    tiles = [{\"row\": row, \"col\": col, \"val\": val} for row, col, val in gen_map(filename)]\n",
    "    moves = [{\"ix\": ix, \"move\": move} for ix, move in enumerate(gen_moves(filename))]\n",
    "\n",
    "    query_ingest_tiles = \"\"\"\n",
    "    UNWIND $tiles AS tile\n",
    "    CREATE (:Tile {row:tile.row, col:tile.col, val:tile.val} )\n",
    "    \"\"\"\n",
    "    query_ingest_moves =\"\"\"\n",
    "    UNWIND $moves AS move\n",
    "    CREATE (:Move {ix: move.ix, move: move.move})\n",
    "    \"\"\"\n",
    "    for q in [query_ingest_tiles, query_ingest_moves]:\n",
    "        gds.run_cypher(q, {\"tiles\":tiles, \"moves\": moves})\n",
    "\n",
    "    gds.run_cypher('CREATE INDEX tile_row IF NOT EXISTS FOR (r:Tile) ON (r.row)')\n",
    "    gds.run_cypher('CREATE INDEX tile_col IF NOT EXISTS FOR (r:Tile) ON (r.col)')\n",
    "    gds.run_cypher('CREATE INDEX tile_val IF NOT EXISTS FOR (r:Tile) ON (r.val)')\n",
    "    gds.run_cypher('CREATE INDEX tile_col_row IF NOT EXISTS FOR (r:Tile) ON (r.col, r.row)')\n",
    "    gds.run_cypher('CREATE INDEX tile_all IF NOT EXISTS FOR (r:Tile) ON (r.col, r.row, r.val)')\n",
    "    gds.run_cypher('CREATE INDEX next_t_move FOR ()-[r:NEXT_TO]-() ON (r.move)')\n",
    "\n",
    "\n",
    "    gds.run_cypher(\"\"\"\n",
    "    MATCH (t:Tile)\n",
    "    WITH t.row AS row_num, t ORDER BY t.col\n",
    "    WITH row_num, collect(t) AS row\n",
    "    CALL apoc.nodes.link(row, 'EAST')\n",
    "    \"\"\")\n",
    "\n",
    "    gds.run_cypher(\"\"\"\n",
    "    MATCH (t:Tile)\n",
    "    WITH t.col AS col_num, t ORDER BY t.row\n",
    "    WITH col_num, collect(t) AS col\n",
    "    CALL apoc.nodes.link(col, 'SOUTH')\n",
    "    \"\"\")\n",
    "\n",
    "    gds.run_cypher(\"\"\"\n",
    "    MATCH (a)-[r:EAST]->(b)\n",
    "    MERGE (a)-[:NEXT_TO {move: '>'}]->(b)\n",
    "    MERGE (b)-[:NEXT_TO {move: '<'}]->(a)              \n",
    "    \"\"\")\n",
    "\n",
    "    gds.run_cypher(\"\"\"\n",
    "    MATCH (a)-[r:SOUTH]->(b)\n",
    "    MERGE (a)-[:NEXT_TO {move: 'v'}]->(b)\n",
    "    MERGE (b)-[:NEXT_TO {move: '^'}]->(a)\n",
    "    \"\"\")\n",
    "\n",
    "    gds.run_cypher(\"\"\"\n",
    "    MATCH (m:Move)\n",
    "    WITH m ORDER BY m.ix\n",
    "    WITH collect(m) AS seq\n",
    "    CALL apoc.nodes.link(seq, 'NEXT_MOVE')\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_map():\n",
    "    print(gds.run_cypher(\"\"\"\n",
    "    MATCH (t:Tile)\n",
    "    ORDER BY  t.col\n",
    "    WITH t.row AS row_num, collect(t) AS tile_row\n",
    "    WITH row_num, apoc.text.join([x IN tile_row | x.val],'') AS row\n",
    "    ORDER BY row_num\n",
    "    WITH collect(row) AS rows\n",
    "    WITH apoc.text.join(rows, '\\n') AS map\n",
    "    RETURN map\"\"\").iloc[0]['map'])\n",
    "\n",
    "    print(gds.run_cypher(\"\"\"\n",
    "    OPTIONAL MATCH (m:Move&!Processed)\n",
    "    WHERE NOT EXISTS {(:Move&!Processed)-[:NEXT_MOVE]->(m)}\n",
    "    RETURN m.move AS move\"\"\").iloc[0]['move'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= \"input.txt\"\n",
    "ingest(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer (rules, params={}, verbose=False):\n",
    "    \"\"\"\n",
    "    This is a function you can use if you want to run a set of inference rules\n",
    "    until a convergence is reached. why not use it in a RDF-like reasoning context?\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        any_update = False\n",
    "        for rule in rules:\n",
    "            with driver.session(database=\"neo4j\") as session:\n",
    "                result = session.run(rule, params)\n",
    "                if verbose:\n",
    "                    print_map()\n",
    "            any_new_update = result.consume().counters._contains_updates\n",
    "            any_update = any_update or any_new_update\n",
    "        if not any_update:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_query = \"\"\"\n",
    "MATCH (m:Move&!Processed) WHERE NOT EXISTS {(:Move&!Processed)-[:NEXT_MOVE]->(m)}\n",
    "SET m:Processed\n",
    "WITH m\n",
    "MATCH (robot:Tile {val:'@'})\n",
    "MATCH path = (robot)(\n",
    "(xi WHERE xi.val IN ['O', '@'])-[:NEXT_TO {move: m.move}]->(y)\n",
    ")*(free:Tile {val: '.'})\n",
    "WITH m, robot, xi[1] AS new_robot, xi[-1] AS last_occupied, free, path\n",
    "SET free.val = last_occupied.val, new_robot.val = '@', robot.val = '.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer([move_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.run_cypher(\"\"\"\n",
    "MATCH (O:Tile {val:'O'})\n",
    "WITH collect(O.row * 100 + O.col) AS gpss\n",
    "RETURN reduce (acc = 0, gps IN gpss | acc+gps) AS part1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer (rules, params={}, verbose=False):\n",
    "    \"\"\"\n",
    "    This is a function you can use if you want to run a set of inference rules\n",
    "    until a convergence is reached. why not use it in a RDF-like reasoning context?\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        any_update = False\n",
    "        for rule in rules:\n",
    "            if verbose:\n",
    "                print(rule)\n",
    "            with driver.session(database=\"neo4j\") as session:\n",
    "                result = session.run(rule, params)\n",
    "                if verbose:\n",
    "                    print(result)\n",
    "            any_new_update = result.consume().counters._contains_updates\n",
    "            if verbose and any_new_update:\n",
    "                print_map()\n",
    "            any_update = any_update or any_new_update\n",
    "        if not any_update:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_2(filename):\n",
    "    clean()\n",
    "\n",
    "    tiles = [{\"row\": row, \"col\": col, \"val\": val} for row, col, val in gen_map_part2(filename)]\n",
    "    moves = [{\"ix\": ix, \"move\": move} for ix, move in enumerate(gen_moves(filename))]\n",
    "\n",
    "    query_ingest_tiles = \"\"\"\n",
    "    UNWIND $tiles AS tile\n",
    "    CREATE (:Tile {row:tile.row, col:tile.col, val:tile.val} )\n",
    "    \"\"\"\n",
    "    query_ingest_moves =\"\"\"\n",
    "    UNWIND $moves AS move\n",
    "    CREATE (:Move {ix: move.ix, move: move.move})\n",
    "    \"\"\"\n",
    "    for q in [query_ingest_tiles, query_ingest_moves]:\n",
    "        gds.run_cypher(q, {\"tiles\":tiles, \"moves\": moves})\n",
    "\n",
    "    gds.run_cypher('CREATE INDEX tile_row IF NOT EXISTS FOR (r:Tile) ON (r.row)')\n",
    "    gds.run_cypher('CREATE INDEX tile_col IF NOT EXISTS FOR (r:Tile) ON (r.col)')\n",
    "    gds.run_cypher('CREATE INDEX tile_val IF NOT EXISTS FOR (r:Tile) ON (r.val)')\n",
    "    gds.run_cypher('CREATE INDEX tile_col_row IF NOT EXISTS FOR (r:Tile) ON (r.col, r.row)')\n",
    "    gds.run_cypher('CREATE INDEX tile_all IF NOT EXISTS FOR (r:Tile) ON (r.col, r.row, r.val)')\n",
    "    gds.run_cypher('CREATE INDEX next_t_move FOR ()-[r:NEXT_TO]-() ON (r.move)')\n",
    "\n",
    "\n",
    "    gds.run_cypher(\"\"\"\n",
    "    MATCH (t:Tile)\n",
    "    WITH t.row AS row_num, t ORDER BY t.col\n",
    "    WITH row_num, collect(t) AS row\n",
    "    CALL apoc.nodes.link(row, 'EAST')\n",
    "    \"\"\")\n",
    "\n",
    "    gds.run_cypher(\"\"\"\n",
    "    MATCH (t:Tile)\n",
    "    WITH t.col AS col_num, t ORDER BY t.row\n",
    "    WITH col_num, collect(t) AS col\n",
    "    CALL apoc.nodes.link(col, 'SOUTH')\n",
    "    \"\"\")\n",
    "\n",
    "    gds.run_cypher(\"\"\"\n",
    "    MATCH (a)-[r:EAST]->(b)\n",
    "    MERGE (a)-[:NEXT_TO {move: '>'}]->(b)\n",
    "    MERGE (b)-[:NEXT_TO {move: '<'}]->(a)              \n",
    "    \"\"\")\n",
    "\n",
    "    gds.run_cypher(\"\"\"\n",
    "    MATCH (a)-[r:SOUTH]->(b)\n",
    "    MERGE (a)-[:NEXT_TO {move: 'v'}]->(b)\n",
    "    MERGE (b)-[:NEXT_TO {move: '^'}]->(a)\n",
    "    \"\"\")\n",
    "\n",
    "    gds.run_cypher(\"\"\"\n",
    "    MATCH (m:Move)\n",
    "    WITH m ORDER BY m.ix\n",
    "    WITH collect(m) AS seq\n",
    "    CALL apoc.nodes.link(seq, 'NEXT_MOVE')\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_map():\n",
    "    print(gds.run_cypher(\"\"\"\n",
    "    MATCH (t:Tile)\n",
    "    ORDER BY  t.col\n",
    "    WITH t.row AS row_num, collect(t) AS tile_row\n",
    "    WITH row_num, apoc.text.join([x IN tile_row | CASE x:Movable WHEN TRUE THEN 'O' ELSE x.val END],'') AS row\n",
    "    ORDER BY row_num\n",
    "    WITH collect(row) AS rows\n",
    "    WITH apoc.text.join(rows, '\\n') AS map\n",
    "    RETURN map\"\"\").iloc[0]['map'])\n",
    "\n",
    "    print(gds.run_cypher(\"\"\"\n",
    "    OPTIONAL MATCH (m:Move&Processed)\n",
    "    WHERE size(labels(m)) > 2\n",
    "    RETURN m.move AS move, labels(m) AS state\"\"\").iloc[0][['move','state']])\n",
    "\n",
    "    print(gds.run_cypher(\"\"\"\n",
    "    MATCH (m:Moveable)\n",
    "    RETURN count(m) AS moveables\"\"\").iloc[0][['moveables']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= \"input.txt\"\n",
    "ingest_2(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"\"\"\n",
    "MATCH (m:Move&!Processed)\n",
    "WHERE NOT EXISTS {(:Updated|Moveable|CleanOrder|FreeOrder|MovingOrder|Current)}\n",
    "AND NOT EXISTS {(:Move&!Processed)-[:NEXT_MOVE]->(m)}\n",
    "WITH m\n",
    "SET m:Current, m:Processed\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "MATCH (m:Move&Current)\n",
    "MATCH (robot:Tile {val:'@'})\n",
    "SET robot:Moveable\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "MATCH (m:Move&Current)\n",
    "MATCH (:Moveable)-[:NEXT_TO {move:m.move}]->(pushed WHERE pushed.val in ['[',']'])\n",
    "SET pushed:Moveable\n",
    "WITH pushed\n",
    "OPTIONAL CALL (pushed) {\n",
    "    MATCH (pushed {val:'['})-[:EAST]->(same_box)\n",
    "    SET same_box:Moveable\n",
    "}\n",
    "WITH pushed\n",
    "OPTIONAL CALL (pushed) {\n",
    "    MATCH (same_box)-[:EAST]->(pushed {val:']'})\n",
    "    SET same_box:Moveable\n",
    "}\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "MATCH (m:Move&Current)\n",
    "WHERE EXISTS {(:Moveable)-[:NEXT_TO {move:m.move}]->(pushed WHERE pushed.val = '#')}\n",
    "REMOVE m:Current\n",
    "WITH m\n",
    "MATCH (mvbl:Moveable)\n",
    "REMOVE mvbl:Moveable\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "MATCH (m:Move&Current)\n",
    "WHERE NOT EXISTS {(:Moveable)-[:NEXT_TO {move:m.move}]->(pushed:!Moveable WHERE pushed.val IN ['#', '[', ']'])}\n",
    "REMOVE m:Current\n",
    "SET m:MovingOrder\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "MATCH (m:Move&MovingOrder)\n",
    "REMOVE m:MovingOrder\n",
    "SET m:FreeOrder\n",
    "WITH m\n",
    "MATCH (source:Moveable)-[:NEXT_TO {move:m.move}]->(target)\n",
    "WITH source.val AS new_val, target \n",
    "SET target.val = new_val, target:Updated\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "MATCH (m:Move:FreeOrder)\n",
    "REMOVE m:FreeOrder\n",
    "SET m:CleanOrder\n",
    "WITH m\n",
    "MATCH (source:Moveable&!Updated)\n",
    "SET source.val = '.'\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "MATCH (m:Move:CleanOrder)\n",
    "WITH m, ['Updated','Moveable','CleanOrder','FreeOrder','MovingOrder','Current'] AS labels\n",
    "UNWIND labels AS l\n",
    "MATCH (n:$(l))\n",
    "REMOVE n:$(l)\n",
    "\"\"\"]\n",
    "\n",
    "#infer(queries, {}, True) #verbose\n",
    "infer(queries, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     part1\n",
       "0  1467145"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher(\"\"\"\n",
    "MATCH (O:Tile {val:'['})\n",
    "WITH collect(O.row * 100 + O.col) AS gpss\n",
    "RETURN reduce (acc = 0, gps IN gpss | acc+gps) AS part1\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
